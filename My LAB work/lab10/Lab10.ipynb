{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0c2087",
   "metadata": {},
   "source": [
    "# Lab 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a598aa",
   "metadata": {},
   "source": [
    "# [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand/data)\n",
    "\n",
    "**Task**: You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d3c50e",
   "metadata": {},
   "source": [
    "**Dataset Features:**\n",
    "\n",
    "- *datetime* - hourly date + timestamm\n",
    "- *season* -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    "- *holiday* - whether the day is considered a holiday\n",
    "- *workingday* - whether the day is neither a weekend nor holiday\n",
    "- *weather* - \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "- *temp* - temperature in Celsius\n",
    "- *atemp* - \"feels like\" temperature in Celsius\n",
    "- *humidity* - relative humidity\n",
    "- *windspeed* - wind speed\n",
    "- *casual* - number of non-registered user rentals initiated\n",
    "- *registered* - number of registered user rentals initiated\n",
    "- *count* - number of total rentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f8361",
   "metadata": {},
   "source": [
    "## Think About the Problem First\n",
    "\n",
    "Try to generate hypothesis. For example, since we have `datetime` feature, we can extract the hour because bike demand may vary depending on the specific time of the day. Next, there might be a relation between the weekday and the demand. For example, in weekends, there might be less demand of bikes. Similary, rainy weather or high temperature might reduce the demand of bikes, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb99ea",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a35ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso, ElasticNet\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f81393",
   "metadata": {},
   "source": [
    "### Load and Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/bike-sharing/train.csv')\n",
    "test_data = pd.read_csv('./data/bike-sharing/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc661440",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Shape: ', train_data.shape)\n",
    "print('Test Shape: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef57546",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.iloc[:, 0:9]\n",
    "Y = train_data['count']\n",
    "\n",
    "print('Train X Shape: ', X.shape)\n",
    "print('Train Y Shape: ', Y.shape)\n",
    "print('Test Shape: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad86a985",
   "metadata": {},
   "source": [
    "### Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5bd29",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b74e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(Y, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(np.log(Y), kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb75454",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(X.season, bins=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e22c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(X.temp, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a2c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(X.atemp, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(X.windspeed, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a533741",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(X.humidity, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355be4d",
   "metadata": {},
   "source": [
    "### Preprocessing & Feature Engineering with Pipeline\n",
    "\n",
    "To learn more about `Pipeline`, you can try the following resources:\n",
    "\n",
    "- [ML Data Pipelines with Custom Transformers in Python](https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65)\n",
    "- [How to Transform Target Variables for Regression in Python](https://machinelearningmastery.com/how-to-transform-target-variables-for-regression-with-scikit-learn/)\n",
    "- [Using scikit-learn Pipelines and FeatureUnions](http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html)\n",
    "- [Pre-Process Data like a Pro: Intro to Scikit-Learn Pipelines](https://towardsdatascience.com/clean-efficient-data-pipelines-with-pythons-sklearn-2472de04c0ea)\n",
    "\n",
    "However, a complete understanding of data transformations can be found in [`sklearn`](https://scikit-learn.org/stable/data_transforms.html) documentation. I urge you all to visit this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ProcessDateTime(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print('Transforming datetime...')\n",
    "        \n",
    "        x_copy = X.copy()\n",
    "        x_copy['month'] = x_copy.datetime.apply(lambda x : calendar.month_name[datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").weekday()])\n",
    "        x_copy['weekday'] = x_copy.datetime.apply(lambda x : calendar.day_name[datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").weekday()])\n",
    "        x_copy['hour'] = x_copy.datetime.apply(lambda x : datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").hour)\n",
    "        x_copy['minute'] = x_copy.datetime.apply(lambda x : datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\").minute)\n",
    "        x_copy = x_copy.drop(['datetime'], axis=1)\n",
    "        \n",
    "        return x_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69efbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('datetime', ProcessDateTime())\n",
    "])\n",
    "\n",
    "pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a631b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessSeasonWeather(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print('Transforming season and weather...')\n",
    "        x_copy = X.copy()\n",
    "        x_copy['season'] = x_copy['season'].map({\n",
    "            1: 'Spring',\n",
    "            2: 'Summer',\n",
    "            3: 'Fall',\n",
    "            4: 'Winter'\n",
    "        })\n",
    "        x_copy['weather'] = x_copy['weather'].map({\n",
    "            1: \"Clear+FewClouds+PartlyCloudy,PartlyCloudy\",\n",
    "            2: \"Mist+Cloudy,Mist+BrokenClouds,Mist+FewClouds,Mist\",\n",
    "            3: \"LightSnow,LightRain+Thunderstorm+ScatteredClouds,LightRain+ScatteredClouds\",\n",
    "            4: \"HeavyRain+IcePallets+Thunderstorm+Mist,Snow+Fog\" \n",
    "        })\n",
    "        return x_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58eedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('datetime', ProcessDateTime()),\n",
    "    ('seasonweather', ProcessSeasonWeather())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52161d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEncoding(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print('Dummy encoding...')\n",
    "        x_copy = X.copy()\n",
    "        x_copy = pd.get_dummies(x_copy)\n",
    "        return x_copy\n",
    "\n",
    "    \n",
    "class RemoveFeature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=[]):\n",
    "        self._features = features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print('Removing features...')\n",
    "        x_copy = X.copy()\n",
    "        for f in self._features:\n",
    "            if f in x_copy.columns:\n",
    "                x_copy = x_copy.drop([f], axis=1)\n",
    "        return x_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('datetime', ProcessDateTime()),\n",
    "    ('seasonweather', ProcessSeasonWeather()),\n",
    "    ('dummyencode', DummyEncoding()),\n",
    "    ('removefeature', RemoveFeature(features=['windspeed']))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89932a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('datetime', ProcessDateTime()),\n",
    "    ('seasonweather', ProcessSeasonWeather()),\n",
    "    ('dummyencode', DummyEncoding()),\n",
    "    ('removefeature', RemoveFeature(features=['windspeed'])),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe872eb",
   "metadata": {},
   "source": [
    "**Why we did not fit test data like we did form train data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('datetime', ProcessDateTime()),\n",
    "    ('seasonweather', ProcessSeasonWeather()),\n",
    "    ('dummyencode', DummyEncoding()),\n",
    "    ('removefeature', RemoveFeature(['windspeed'])),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "pipeline.fit(X)\n",
    "X = pipeline.transform(X)\n",
    "X_test = pipeline.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a900de1",
   "metadata": {},
   "source": [
    "**Try using `StandardScaler` instead of `MinMaxScaler` and see what happens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af016dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f689433",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf593ca8",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "sgd = SGDRegressor()\n",
    "rr = Ridge()\n",
    "ls = Lasso()\n",
    "en = ElasticNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f344b",
   "metadata": {},
   "source": [
    "A model with some regularization typically performs better than a model without any regularization, so you should generally prefer Ridge Regression over plain Linear Regression.\n",
    "\n",
    "Lasso Regression uses an $l_1$ penalty, which tends to push the weights down to exactly zero. This leads to sparse models, where all weights are zero except for the most important weights. This is a way to perform feature\n",
    "selection automatically, which is good if you suspect that only a few features actually matter. When you are not\n",
    "sure, you should prefer Ridge Regression.\n",
    "\n",
    "Elastic Net is generally preferred over Lasso since Lasso may behave erratically in some cases (when several features are strongly correlated or when there are more features than training instances). However, it does add an extra hyperparameter to tune. If you want Lasso without the erratic behavior, you can just use Elastic Net with an l1_ratio close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69255649",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663dfeb",
   "metadata": {},
   "source": [
    "### Hyperparamter Tuning Using GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afaf8d",
   "metadata": {},
   "source": [
    "![](./images/cv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e48f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=27)\n",
    "\n",
    "grid_ridge_lasso = {\n",
    "    'alpha': np.arange(0, 1, 0.05)\n",
    "}\n",
    "\n",
    "grid_elastic = {\n",
    "    'alpha': np.arange(0, 1, 0.05),\n",
    "    'l1_ratio': np.arange(0, 1, 0.05)\n",
    "}\n",
    "\n",
    "lr_score = cross_val_score(lr, X, np.log(Y+0.0001), cv=cv, scoring='neg_mean_squared_log_error')\n",
    "sgd_score = cross_val_score(sgd, X, np.log(Y+0.0001), cv=cv, scoring='neg_mean_squared_log_error')\n",
    "\n",
    "rr_search = GridSearchCV(rr, grid_ridge_lasso, cv=cv, scoring='neg_mean_squared_log_error')\n",
    "rr_score = rr_search.fit(X, np.log(Y+0.0001))\n",
    "\n",
    "ls_search = GridSearchCV(ls, grid_ridge_lasso, cv=cv, scoring='neg_mean_squared_log_error')\n",
    "ls_score = ls_search.fit(X, np.log(Y+0.0001))\n",
    "\n",
    "en_search = GridSearchCV(en, grid_elastic, cv=cv, scoring='neg_mean_squared_log_error')\n",
    "en_score = en_search.fit(X, np.log(Y+0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(lr_score))\n",
    "print(np.mean(sgd_score))\n",
    "\n",
    "print(rr_score.best_score_)\n",
    "print(ls_score.best_score_)\n",
    "print(en_score.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a36a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.exp(rr_score.best_estimator_.predict(X_test))\n",
    "predictions = predictions.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b138af",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951c1055",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'datetime': test_data.datetime,\n",
    "    'count': predictions\n",
    "}).to_csv('./output/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97aa0dc",
   "metadata": {},
   "source": [
    "**Kaggle Score:** 1.02292 <br/>\n",
    "**Winning Score:** 0.3375"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e4efc",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59f782",
   "metadata": {},
   "source": [
    "![Metrics](./images/metrics.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a697b1",
   "metadata": {},
   "source": [
    "**Let's start by defining some dummy labels and their corresponding predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = np.array([1, 2, 3, 4])\n",
    "label1 = np.array([2, 3, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae830250",
   "metadata": {},
   "source": [
    "**The most obvious and naive way to measure error is to simply calculate the difference between them and sum them up. However, simple difference between the predictions and the labels does not always show the actual picture. For example, does the error `0` below show the actual state of the model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dfdb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pred1 - label1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e876e93",
   "metadata": {},
   "source": [
    "**The problem was that when we were adding the errors, negative and positive errors like `-5` and `5` were cancelling themselves. However, `-5` means our prediction was `5` units short to the actual value. In contrast, `5` means our prediction was `5` units up from the actual value. If we get a total of `0` error by adding themselves, it does not make sense. Taking absolute difference can solve this problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebddd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute difference\n",
    "np.sum(np.abs(pred1 - label1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c10e3a",
   "metadata": {},
   "source": [
    "**Now `-5` will convert into its absolute value `5` and we can add it to the other `5` to get a total error of `10`. Now it gives us the actual picture of the performance of our model. But we can not compare absolute error between two different sized datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc5d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = np.array([1, 2, 3, 4, 1, 2])\n",
    "label2 = np.array([2, 3, 2, 3, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1567512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute difference\n",
    "np.sum(np.abs(pred2 - label2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281c616",
   "metadata": {},
   "source": [
    "**Here we just added first to data again at the end. Basically, this is the same prediction like the previous but with 2 extra data. But we are getting extra error. This is because we are adding the individual errors. So, the more data we have the more error will it generate, which is not fair. To compare between different sized datasets, we can simply take the mean.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34870ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absolute difference\n",
    "print(mean_absolute_error(pred1, label1))\n",
    "print(mean_absolute_error(pred2, label2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66771828",
   "metadata": {},
   "source": [
    "**Another option to mitigate the problem of taking simple difference is to square the difference so that there will be no negative number to cancel out each other. This has another added advantage that it penalizes large errors more (Compare the difference between 2 and 4, and $2^2$ and $4^2$). Also, squared terms are good for calculating derivatives and thus useful in gradient descent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8401e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = np.array([1, 2, 3, 4, 1, 2])\n",
    "label3 = np.array([2, 3, 2, 7, 2, 3])\n",
    "\n",
    "pred4 = np.array([1, 2, 3, 4, 1, 2])\n",
    "label4 = np.array([2, 3, 2, 10, 2, 3])\n",
    "\n",
    "pred5 = np.array([1, 2, 3, 4, 1, 2])\n",
    "label5 = np.array([2, 3, 2, 13, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da28a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absolute difference\n",
    "print(mean_absolute_error(pred3, label3))\n",
    "print(mean_absolute_error(pred4, label4))\n",
    "print(mean_absolute_error(pred5, label5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9fa9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared difference\n",
    "print(mean_squared_error(pred3, label3))\n",
    "print(mean_squared_error(pred4, label4))\n",
    "print(mean_squared_error(pred5, label5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3d1e7",
   "metadata": {},
   "source": [
    "**Squaring error terms also squares the units. For example, if we have an error of, say, $\\$15$ dollars, it will be $\\$225$ $dollar^2$ after squaring. But there is nothing such as $dollar^2$. So, we take the root of the squared error to get back the original unit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root mean square error\n",
    "print(np.sqrt(mean_squared_error(pred3, label3)))\n",
    "print(np.sqrt(mean_squared_error(pred4, label4)))\n",
    "print(np.sqrt(mean_squared_error(pred5, label5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cb0b6",
   "metadata": {},
   "source": [
    "**Squaring and amplifying large errors are not what always expected. There can be contexts where you would want to ignore large mistakes.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824cc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([60, 70, 80, 90])\n",
    "label = np.array([57, 74, 79, 89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(pred, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ba6bf",
   "metadata": {},
   "source": [
    "**Next, we have added an outlier, where the real label is `7`, but the prediction is `85`. As usual, `mean squared error` penalizes large mistakes more.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([60, 70, 80, 90, 85])\n",
    "label = np.array([57, 74, 79, 89, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22343a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(pred, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00af17b",
   "metadata": {},
   "source": [
    "**Mean squared log error (MSLE) or root mean squared log error (RMSLE) can avoid this situation. MSLE is basically MSE with the exception that we are calculating difference between $log(prediction)$ and $log(label)$ instead of plain $prediction$ and $label$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be337842",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_log_error(pred, label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
